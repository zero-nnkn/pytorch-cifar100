{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s773UhN8Y5t7"
      },
      "source": [
        "## Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnuGo5tyZAWx",
        "outputId": "d6096b38-7965-4b19-f836-c5b79aef40e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/MyDrive/Course/cinnamon/pytorch-compression-cifar100\n"
          ]
        }
      ],
      "source": [
        "# For Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeP_pHm1Y5uB"
      },
      "outputs": [],
      "source": [
        "# !gdown 1zFsUJWH86xxB5-aQ66GJ9WyY8bkwrG0p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rivgjNwjY5uD"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from conf import settings\n",
        "from utils import get_test_dataloader\n",
        "from benchmark import evaluate, create_torch_profile, get_model_size\n",
        "\n",
        "\n",
        "def create_report(model, test_loader, device, dummy_input=None):\n",
        "    if dummy_input is None:\n",
        "        dummy_input = torch.randn(1, 3, 32, 32, device=device)\n",
        "\n",
        "    # Error\n",
        "    top1_err, top5_err, t = evaluate(model, test_loader, device)\n",
        "    print('Top 1 err:', top1_err)\n",
        "    print('Top 5 err:', top5_err)\n",
        "    print(f'Time per image: {t} (ms)')\n",
        "\n",
        "    # Size\n",
        "    model_size = get_model_size(model)\n",
        "    print(f\"Model size: {model_size/1e3} (MB)\")\n",
        "\n",
        "\n",
        "    # Torch profile\n",
        "    create_torch_profile(model, dummy_input, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9ZyK-_dY5uF",
        "outputId": "3aae749f-9954-4f7c-ac8b-03d23f5ba506"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "device = 'cpu'\n",
        "\n",
        "test_loader = get_test_dataloader(\n",
        "    settings.CIFAR100_TRAIN_MEAN,\n",
        "    settings.CIFAR100_TRAIN_STD,\n",
        "    num_workers=4,\n",
        "    batch_size=16,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8Z9KYHmY5uI",
        "outputId": "adacd432-a37a-4130-efa5-c9af1d5383e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from models.vgg import vgg19_bn\n",
        "\n",
        "baseline = vgg19_bn()\n",
        "baseline.load_state_dict(torch.load('vgg19-61-best.pth', map_location='cpu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HwgM09wY5uJ",
        "outputId": "8465d531-8f7a-46d1-c227-37077d4c2b3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 1 err: 0.3230999708175659\n",
            "Top 5 err: 0.11979997158050537\n",
            "Time per image: 21.051048493385313 (ms)\n",
            "Model size: 157.384365 (MB)\n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                      aten::empty         0.71%     280.000us         0.71%     280.000us       2.188us       5.47 Mb       5.47 Mb           128  \n",
            "                     aten::conv2d         0.65%     256.000us        68.94%      27.241ms       1.703ms       1.16 Mb           0 b            16  \n",
            "                aten::convolution         0.71%     282.000us        68.29%      26.985ms       1.687ms       1.16 Mb           0 b            16  \n",
            "               aten::_convolution         1.63%     645.000us        67.58%      26.703ms       1.669ms       1.16 Mb           0 b            16  \n",
            "                 aten::batch_norm         0.31%     122.000us         9.60%       3.795ms     237.188us       1.16 Mb       8.00 Kb            16  \n",
            "     aten::_batch_norm_impl_index         7.00%       2.765ms         9.44%       3.732ms     233.250us       1.16 Mb           0 b            16  \n",
            "          aten::native_batch_norm         1.95%     769.000us         2.35%     930.000us      58.125us       1.16 Mb     -39.00 Kb            16  \n",
            "                 aten::empty_like         0.12%      49.000us         0.27%     108.000us       6.750us       1.16 Mb           0 b            16  \n",
            "                aten::thnn_conv2d         0.14%      56.000us        58.96%      23.298ms       1.664ms     800.00 Kb           0 b            14  \n",
            "       aten::_slow_conv2d_forward        52.13%      20.599ms        58.82%      23.242ms       1.660ms     800.00 Kb      -3.78 Mb            14  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 39.515ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "create_report(baseline, test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCikoqCrY5uK"
      },
      "source": [
        "## Quantize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHXd5gQwY5uK"
      },
      "source": [
        "### Post-Training Dynamic/Weight-only Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EL3S4fPHY5uL"
      },
      "outputs": [],
      "source": [
        "from torch.quantization import quantize_dynamic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qBjZ7T2Y5uM",
        "outputId": "5574c38e-d60e-4cbd-fa2f-fe626118d0fe"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 1 err: 0.3230999708175659\n",
            "Top 5 err: 0.11979997158050537\n",
            "Time per image: 20.80197432041168 (ms)\n",
            "Model size: 157.38644699999998 (MB)\n",
            "----------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                              Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "----------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                       aten::empty         1.16%     485.000us         1.16%     485.000us       3.702us       5.48 Mb       5.48 Mb           131  \n",
            "                      aten::conv2d         0.34%     141.000us        82.03%      34.256ms       2.141ms       1.16 Mb           0 b            16  \n",
            "                 aten::convolution         1.47%     612.000us        81.69%      34.115ms       2.132ms       1.16 Mb           0 b            16  \n",
            "                aten::_convolution         1.11%     462.000us        80.23%      33.503ms       2.094ms       1.16 Mb           0 b            16  \n",
            "                  aten::batch_norm         0.25%     104.000us         4.05%       1.693ms     105.812us       1.16 Mb           0 b            16  \n",
            "      aten::_batch_norm_impl_index         0.48%     202.000us         3.80%       1.589ms      99.312us       1.16 Mb           0 b            16  \n",
            "           aten::native_batch_norm         2.54%       1.059ms         3.16%       1.320ms      82.500us       1.16 Mb     -43.00 Kb            16  \n",
            "                  aten::empty_like         0.17%      71.000us         0.40%     166.000us      10.375us       1.16 Mb           0 b            16  \n",
            "                     aten::resize_         0.25%     105.000us         0.25%     105.000us       6.176us     832.39 Kb     832.39 Kb            17  \n",
            "                 aten::thnn_conv2d         0.24%     101.000us        64.84%      27.076ms       1.934ms     800.00 Kb           0 b            14  \n",
            "----------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 41.761ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_quantized_dynamic_float16 = quantize_dynamic(\n",
        "    model=baseline, qconfig_spec={torch.nn.Linear}, dtype=torch.float16,\n",
        ")\n",
        "create_report(model_quantized_dynamic_float16, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Af8sVBxqY5uM",
        "outputId": "e4ba380f-660a-4213-d2ba-fb6f44c43d68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 1 err: 0.32260000705718994\n",
            "Top 5 err: 0.11970001459121704\n",
            "Time per image: 10.717263984680175 (ms)\n",
            "Model size: 99.534671 (MB)\n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                      aten::empty         4.12%     640.000us         4.12%     640.000us       4.776us       5.53 Mb       5.53 Mb           134  \n",
            "                 aten::empty_like         0.30%      47.000us         0.68%     105.000us       5.526us       1.19 Mb           0 b            19  \n",
            "                     aten::conv2d         2.69%     419.000us        76.13%      11.838ms     739.875us       1.16 Mb           0 b            16  \n",
            "                aten::convolution         1.56%     242.000us        73.44%      11.419ms     713.688us       1.16 Mb           0 b            16  \n",
            "               aten::_convolution         0.96%     149.000us        71.88%      11.177ms     698.562us       1.16 Mb           0 b            16  \n",
            "                 aten::batch_norm         1.81%     281.000us         9.68%       1.505ms      94.062us       1.16 Mb      64.00 Kb            16  \n",
            "     aten::_batch_norm_impl_index         2.12%     330.000us         8.22%       1.278ms      79.875us       1.16 Mb           0 b            16  \n",
            "          aten::native_batch_norm         4.91%     763.000us         5.89%     916.000us      57.250us       1.16 Mb     -32.00 Kb            16  \n",
            "                aten::thnn_conv2d         0.30%      46.000us        62.00%       9.641ms     688.643us     800.00 Kb           0 b            14  \n",
            "       aten::_slow_conv2d_forward        55.29%       8.597ms        61.71%       9.595ms     685.357us     800.00 Kb      -3.89 Mb            14  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 15.549ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_quantized_dynamic_int8 = quantize_dynamic(\n",
        "    model=baseline, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8,\n",
        ")\n",
        "create_report(model_quantized_dynamic_int8, test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkKZvxfwY5uM"
      },
      "source": [
        "### Post-Training Static Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "k_LdSD4mY5uN"
      },
      "outputs": [],
      "source": [
        "from torch.nn.utils.fusion import fuse_conv_bn_eval\n",
        "\n",
        "\n",
        "def fuse_all_conv_bn(model):\n",
        "    \"\"\"\n",
        "    Fuses all consecutive Conv2d and BatchNorm2d layers.\n",
        "    License: Copyright Zeeshan Khan Suri, CC BY-NC 4.0\n",
        "    \"\"\"\n",
        "    stack = []\n",
        "    for name, module in model.named_children(): # immediate children\n",
        "        if list(module.named_children()): # is not empty (not a leaf)\n",
        "            fuse_all_conv_bn(module)\n",
        "\n",
        "        if isinstance(module, nn.BatchNorm2d):\n",
        "            if isinstance(stack[-1][1], nn.Conv2d):\n",
        "                setattr(model, stack[-1][0], fuse_conv_bn_eval(stack[-1][1], module))\n",
        "                setattr(model, name, nn.Identity())\n",
        "        else:\n",
        "            stack.append((name, module))\n",
        "\n",
        "def ptq(model, sample_loader, device='cpu', backend='fbgemm', fuse_bn=True):\n",
        "    # running on a x86 CPU. Use backend=\"qnnpack\" if running on ARM.\n",
        "    m = deepcopy(model)\n",
        "    m.eval()\n",
        "\n",
        "    # Fuse\n",
        "    if fuse_bn:\n",
        "        fuse_all_conv_bn(m)\n",
        "\n",
        "    # Insert stubs\n",
        "    m = nn.Sequential(\n",
        "        torch.quantization.QuantStub(),\n",
        "        m,\n",
        "        torch.quantization.DeQuantStub()\n",
        "    )\n",
        "\n",
        "    # Prepare\n",
        "    m.qconfig = torch.quantization.get_default_qconfig(backend)\n",
        "    torch.quantization.prepare(m, inplace=True)\n",
        "\n",
        "    # Calibrate\n",
        "    m.to(device)\n",
        "    m.eval()\n",
        "    with torch.no_grad():\n",
        "        for data, target in sample_loader:\n",
        "            data = data.to(device)\n",
        "            m(data)\n",
        "\n",
        "    # Convert\n",
        "    torch.quantization.convert(m, inplace=True)\n",
        "\n",
        "    return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VoR2PfsY5uN",
        "outputId": "184736db-2cdf-405b-ea41-3cd8d93717d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 1 err: 0.3264999985694885\n",
            "Top 5 err: 0.1226000189781189\n",
            "Time per image: 10.191714763641357 (ms)\n",
            "Model size: 39.726023 (MB)\n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                      aten::empty         0.94%     118.000us         0.94%     118.000us       2.269us       1.13 Mb       1.13 Mb            52  \n",
            "    aten::_empty_affine_quantized         1.43%     179.000us         1.43%     179.000us       4.366us     625.50 Kb     625.50 Kb            41  \n",
            "          quantized::batch_norm2d         2.93%     367.000us         5.04%     632.000us      39.500us     297.50 Kb     -41.50 Kb            16  \n",
            "                quantized::conv2d        69.21%       8.682ms        70.79%       8.881ms     555.062us     296.00 Kb      -1.06 Mb            16  \n",
            "                 aten::empty_like         0.92%     116.000us         1.54%     193.000us       6.031us      43.00 Kb         256 b            32  \n",
            "                 aten::max_pool2d         0.18%      23.000us         0.98%     123.000us      24.600us      30.50 Kb           0 b             5  \n",
            "       aten::quantized_max_pool2d         0.67%      84.000us         0.80%     100.000us      20.000us      30.50 Kb           0 b             5  \n",
            "                quantized::linear        17.86%       2.241ms        18.11%       2.272ms     757.333us       8.10 Kb     -32.39 Kb             3  \n",
            "                    aten::resize_         0.10%      13.000us         0.10%      13.000us       4.333us       8.10 Kb       8.10 Kb             3  \n",
            "        aten::quantize_per_tensor         0.43%      54.000us         0.43%      54.000us      54.000us       3.00 Kb       3.00 Kb             1  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 12.545ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_quantized_static_int8 = ptq(baseline, sample_loader=test_loader, device=device, backend='fbgemm', fuse_bn=False)\n",
        "create_report(model_quantized_static_int8, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE3PyI4PY5uN",
        "outputId": "e52a2e81-79a9-43f1-827a-ca6b65baa123"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\nnkho\\miniconda3\\envs\\cinnamon-practice\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 1 err: 0.32440000772476196\n",
            "Top 5 err: 0.12139999866485596\n",
            "Time per image: 4.995114207267761 (ms)\n",
            "Model size: 39.609398999999996 (MB)\n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                      aten::empty         0.86%      67.000us         0.86%      67.000us       3.350us       1.19 Mb       1.19 Mb            20  \n",
            "    aten::_empty_affine_quantized         2.15%     168.000us         2.15%     168.000us       6.720us     329.50 Kb     329.50 Kb            25  \n",
            "                quantized::conv2d        61.74%       4.824ms        73.23%       5.722ms     357.625us     296.00 Kb      -1.16 Mb            16  \n",
            "                 aten::max_pool2d         0.31%      24.000us         3.11%     243.000us      48.600us      30.50 Kb           0 b             5  \n",
            "       aten::quantized_max_pool2d         2.28%     178.000us         2.80%     219.000us      43.800us      30.50 Kb           0 b             5  \n",
            "                quantized::linear        15.77%       1.232ms        16.23%       1.268ms     422.667us       8.10 Kb     -32.39 Kb             3  \n",
            "                    aten::resize_         0.18%      14.000us         0.18%      14.000us       4.667us       8.10 Kb       8.10 Kb             3  \n",
            "        aten::quantize_per_tensor         0.74%      58.000us         0.74%      58.000us      58.000us       3.00 Kb       3.00 Kb             1  \n",
            "                 aten::contiguous         2.83%     221.000us         9.44%     738.000us     738.000us       3.00 Kb           0 b             1  \n",
            "                      aten::clone         6.48%     506.000us         6.62%     517.000us     517.000us       3.00 Kb           0 b             1  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 7.814ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_quantized_static_fuse_int8 = ptq(baseline, sample_loader=test_loader, device=device, backend='fbgemm', fuse_bn=True)\n",
        "create_report(model_quantized_static_fuse_int8, test_loader, device)\n",
        "# torch.save(model_quantized_static_fuse_int8.state_dict(), 'vgg19_quantized_static_fuse_int8.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgnaU_LsY5uN"
      },
      "source": [
        "## Prune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXvSX4wQY5uO"
      },
      "source": [
        "### Pytorch prunning\n",
        "- Pruning is strictly in research phase and not actually providing any benefits yet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "uJLMidAOY5uO"
      },
      "outputs": [],
      "source": [
        "def measure_module_sparsity(module, weight=True, bias=False, use_mask=False):\n",
        "\n",
        "    num_zeros = 0\n",
        "    num_elements = 0\n",
        "\n",
        "    if use_mask == True:\n",
        "        for buffer_name, buffer in module.named_buffers():\n",
        "            if \"weight_mask\" in buffer_name and weight == True:\n",
        "                num_zeros += torch.sum(buffer == 0).item()\n",
        "                num_elements += buffer.nelement()\n",
        "            if \"bias_mask\" in buffer_name and bias == True:\n",
        "                num_zeros += torch.sum(buffer == 0).item()\n",
        "                num_elements += buffer.nelement()\n",
        "    else:\n",
        "        for param_name, param in module.named_parameters():\n",
        "            if \"weight\" in param_name and weight == True:\n",
        "                num_zeros += torch.sum(param == 0).item()\n",
        "                num_elements += param.nelement()\n",
        "            if \"bias\" in param_name and bias == True:\n",
        "                num_zeros += torch.sum(param == 0).item()\n",
        "                num_elements += param.nelement()\n",
        "\n",
        "    sparsity = num_zeros / num_elements\n",
        "\n",
        "    return num_zeros, num_elements, sparsity\n",
        "\n",
        "\n",
        "def measure_global_sparsity(model,\n",
        "                            weight=True,\n",
        "                            bias=False,\n",
        "                            conv2d_use_mask=False,\n",
        "                            linear_use_mask=False):\n",
        "\n",
        "    num_zeros = 0\n",
        "    num_elements = 0\n",
        "\n",
        "    for module_name, module in model.named_modules():\n",
        "\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "\n",
        "            module_num_zeros, module_num_elements, _ = measure_module_sparsity(\n",
        "                module, weight=weight, bias=bias, use_mask=conv2d_use_mask)\n",
        "            num_zeros += module_num_zeros\n",
        "            num_elements += module_num_elements\n",
        "\n",
        "        elif isinstance(module, torch.nn.Linear):\n",
        "\n",
        "            module_num_zeros, module_num_elements, _ = measure_module_sparsity(\n",
        "                module, weight=weight, bias=bias, use_mask=linear_use_mask)\n",
        "            num_zeros += module_num_zeros\n",
        "            num_elements += module_num_elements\n",
        "\n",
        "    sparsity = num_zeros / num_elements\n",
        "\n",
        "    return sparsity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HoJfmR8VY5uO"
      },
      "outputs": [],
      "source": [
        "import torch.nn.utils.prune as prune\n",
        "\n",
        "\n",
        "def remove_parameters(model, bias=False):\n",
        "    for module_name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Conv2d) and prune.is_pruned(module):\n",
        "            prune.remove(module, \"weight\")\n",
        "            if bias:\n",
        "                prune.remove(module, \"bias\")\n",
        "        elif isinstance(module, torch.nn.Linear) and prune.is_pruned(module):\n",
        "            prune.remove(module, \"weight\")\n",
        "            if bias:\n",
        "                prune.remove(module, \"bias\")\n",
        "\n",
        "\n",
        "def prune_model(model, grouped_pruning, conv2d_prune_amount=0, linear_prune_amount=0):\n",
        "    m = deepcopy(model)\n",
        "\n",
        "    if grouped_pruning:\n",
        "        parameters_to_prune = []\n",
        "        for module_name, module in m.named_modules():\n",
        "            if isinstance(module, torch.nn.Conv2d):\n",
        "                parameters_to_prune.append((module, \"weight\"))\n",
        "        prune.global_unstructured(\n",
        "            parameters_to_prune,\n",
        "            pruning_method=prune.L1Unstructured,\n",
        "            amount=conv2d_prune_amount,\n",
        "        )\n",
        "    else:\n",
        "        for module_name, module in m.named_modules():\n",
        "            if isinstance(module, torch.nn.Conv2d):\n",
        "                prune.l1_unstructured(module,\n",
        "                                        name=\"weight\",\n",
        "                                        amount=conv2d_prune_amount)\n",
        "            elif isinstance(module, torch.nn.Linear):\n",
        "                prune.l1_unstructured(module,\n",
        "                                    name=\"weight\",\n",
        "                                    amount=linear_prune_amount)\n",
        "    remove_parameters(m)\n",
        "    print(\"Sparsity:\", measure_global_sparsity(m))\n",
        "    return m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDKPibnHY5uP",
        "outputId": "bc5ed928-94db-4ef1-89f1-0a6f8e098ac6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sparsity: 0.3056096087489639\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 1 err: 0.3636000156402588\n",
            "Top 5 err: 0.14230000972747803\n",
            "Time per image: 22.443213438987733 (ms)\n",
            "Model size: 157.384365 (MB)\n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                      aten::empty         1.24%     591.000us         1.24%     591.000us       4.617us       5.48 Mb       5.48 Mb           128  \n",
            "                     aten::conv2d         0.36%     174.000us        74.92%      35.719ms       2.232ms       1.16 Mb           0 b            16  \n",
            "                aten::convolution         1.07%     511.000us        74.55%      35.545ms       2.222ms       1.16 Mb           0 b            16  \n",
            "               aten::_convolution         0.82%     393.000us        73.48%      35.034ms       2.190ms       1.16 Mb           0 b            16  \n",
            "                 aten::batch_norm         0.25%     121.000us         4.57%       2.179ms     136.188us       1.16 Mb           0 b            16  \n",
            "     aten::_batch_norm_impl_index         0.57%     272.000us         4.32%       2.058ms     128.625us       1.16 Mb           0 b            16  \n",
            "          aten::native_batch_norm         2.72%       1.295ms         3.53%       1.685ms     105.312us       1.16 Mb     -43.00 Kb            16  \n",
            "                 aten::empty_like         0.24%     115.000us         0.48%     229.000us      14.312us       1.16 Mb           0 b            16  \n",
            "                aten::thnn_conv2d         0.26%     126.000us        64.01%      30.517ms       2.180ms     800.00 Kb           0 b            14  \n",
            "       aten::_slow_conv2d_forward        61.38%      29.266ms        63.74%      30.391ms       2.171ms     800.00 Kb      -3.90 Mb            14  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 47.677ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_pruned_group = prune_model(baseline, grouped_pruning=True, conv2d_prune_amount=0.6)\n",
        "create_report(model_pruned_group, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY_44H-pY5uP",
        "outputId": "e5372c5f-3c67-42e8-e533-2845315be83b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sparsity: 0.3056096596358615\n",
            "Top 1 err: 0.4927999973297119\n",
            "Top 5 err: 0.23309999704360962\n",
            "Time per image: 21.303886651992798 (ms)\n",
            "Model size: 157.384365 (MB)\n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                      aten::empty         5.48%       3.147ms         5.48%       3.147ms      24.586us       5.47 Mb       5.47 Mb           128  \n",
            "                     aten::conv2d         0.25%     144.000us        79.73%      45.805ms       2.863ms       1.16 Mb           0 b            16  \n",
            "                aten::convolution         0.80%     458.000us        79.48%      45.661ms       2.854ms       1.16 Mb           0 b            16  \n",
            "               aten::_convolution         0.62%     358.000us        78.69%      45.203ms       2.825ms       1.16 Mb           0 b            16  \n",
            "                 aten::batch_norm         0.19%     111.000us         3.34%       1.920ms     120.000us       1.16 Mb           0 b            16  \n",
            "     aten::_batch_norm_impl_index         0.43%     245.000us         3.15%       1.809ms     113.062us       1.16 Mb           0 b            16  \n",
            "          aten::native_batch_norm         2.04%       1.170ms         2.55%       1.466ms      91.625us       1.16 Mb     -41.00 Kb            16  \n",
            "                 aten::empty_like         0.13%      75.000us         0.32%     182.000us      11.375us       1.16 Mb           0 b            16  \n",
            "                aten::thnn_conv2d         0.24%     136.000us        70.43%      40.461ms       2.890ms     800.00 Kb           0 b            14  \n",
            "       aten::_slow_conv2d_forward        63.85%      36.681ms        70.20%      40.325ms       2.880ms     800.00 Kb      -3.89 Mb            14  \n",
            "---------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 57.447ms\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_pruned = prune_model(baseline, grouped_pruning=False, conv2d_prune_amount=0.6, linear_prune_amount=0)\n",
        "create_report(model_pruned, test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1qBxMDeY5uP"
      },
      "source": [
        "## ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HF2YQjEojWqu",
        "outputId": "045d104a-c2e7-457c-b2ed-bd0158c43f25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx) (1.23.5)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.7.1)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.14.0\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.5.26)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.11.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Installing collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.15.1\n"
          ]
        }
      ],
      "source": [
        "!pip install onnx\n",
        "!pip install onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Jt-MhQdVY5uQ"
      },
      "outputs": [],
      "source": [
        "import onnx\n",
        "import onnxruntime\n",
        "\n",
        "from benchmark import evaluate_onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "hDLW4d6dY5uQ"
      },
      "outputs": [],
      "source": [
        "def export_onnx(model, save_path, device='cpu', verbose=False):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    x = torch.randn(1, 3, 32, 32, requires_grad=True, device=device)\n",
        "\n",
        "    torch.onnx.export(\n",
        "        model,             # model being run\n",
        "        x,                 # model input (or a tuple for multiple inputs)\n",
        "        save_path,   # where to save the model (can be a file or file-like object)\n",
        "        opset_version=15,  # the ONNX version to export the model to\n",
        "        do_constant_folding=True,  # whether to execute constant folding for optimization\n",
        "        input_names = ['input'],   # the model's input names\n",
        "        output_names = ['output'], # the model's output names\n",
        "        dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n",
        "                    'output' : {0 : 'batch_size'}},\n",
        "        verbose=verbose,\n",
        "    )\n",
        "\n",
        "    # Check\n",
        "    onnx_model = onnx.load(save_path)\n",
        "    onnx.checker.check_model(onnx_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJZH3BvCY5uQ",
        "outputId": "65919f7d-f7f9-40b0-8f3b-30632111aab4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "export_onnx(baseline, save_path='vgg19_bn.onnx', device=device, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jfzBeMZY5uQ",
        "outputId": "a4ce36f0-a412-44f9-ca4b-ac7569683643"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 1 err: 0.3230999708175659\n",
            "Top 5 err: 0.11979997158050537\n",
            "Time per image: 17.320813941955567 (ms)\n"
          ]
        }
      ],
      "source": [
        "ort_session = onnxruntime.InferenceSession(\"vgg19_bn.onnx\")\n",
        "top1_err, top5_err, t = evaluate_onnx(ort_session, test_loader)\n",
        "print('Top 1 err:', top1_err)\n",
        "print('Top 5 err:', top5_err)\n",
        "print(f'Time per image: {t} (ms)')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
